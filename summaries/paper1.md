# Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning

introdução: problema, proposta, contribuições

metodologia: dataset, o que foi usado para visao, o que foi usado para linguagem, o que foi usado para mesclagem de features, o que foi usado para saida.

Resultados: Metricas usadas, trabalhos comparados, uma descrição de como o trabalho atual fica melhor do que os anteriores.

Conclusões: Um parágrafo descrevendo o que foi entendido

## Abstract

## Introduction

Well-annotated Med-VQA datasets are extremely lacking, since it requires medical expertise to construct high-quality datasets, which is both costly and time-consuming.

To our best knowledge, there are only two manually annotated datasets available - VQA-RAD [36] and SLAKE [39].

Moreover, it is impossible to apply popular object-detection-based VQA models such as UpDn [4], Pythia [26], and VL-BERT [57] for Med-VQA, due to the lack of visual object labels and the small size of training data

The paper approach uses unsupervised learning and self-supervised contrastive learning to address the data scarcity problem. There are many such types of unlabeled radiology images available in open-access sources

https://medium.com/@c.michael.yu/what-is-self-supervised-contrastive-learning-df3044d51950
